# 机器学习算法与应用：Scikit-learn、KNN、朴素贝叶斯、决策树与随机森林

图表

代码

```
graph TD
    A[机器学习算法] --> B[监督学习]
    A --> C[无监督学习]
    B --> D[分类算法]
    B --> E[回归算法]
    D --> F[K-近邻]
    D --> G[朴素贝叶斯]
    D --> H[决策树]
    D --> I[随机森林]
    E --> J[线性回归]
    E --> K[岭回归]
```

## 1. Scikit-learn 数据集与估计器

### 1.1 数据集划分

- **训练数据**：用于构建模型（通常70-80%）
- **测试数据**：用于评估模型性能（通常20-30%）

**API**：

python

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

### 1.2 Scikit-learn 数据集接口

| 方法                               | 描述                     |
| :--------------------------------- | :----------------------- |
| `datasets.load_*()`                | 加载小型数据集（内存中） |
| `datasets.fetch_*(data_home=None)` | 下载大型数据集到指定目录 |

**数据集属性**：

python

```
from sklearn.datasets import load_iris
iris = load_iris()
print(iris.data.shape)      # 特征数据 (150, 4)
print(iris.target.shape)    # 标签数据 (150,)
print(iris.feature_names)   # 特征名称
print(iris.target_names)    # 目标类别名称
```

### 1.3 数据集示例

**分类数据集**：

- `load_iris()`：鸢尾花数据集（3类，4特征）
- `load_digits()`：手写数字数据集（10类，64特征）

**回归数据集**：

- `load_diabetes()`：糖尿病数据集（10特征）

## 2. K-近邻算法（KNN）

### 2.1 算法原理

图表

代码

```
graph LR
    A[新样本] --> B[计算与所有样本的欧氏距离]
    B --> C[选择距离最小的K个邻居]
    C --> D[统计K个邻居的类别分布]
    D --> E[将多数类作为预测结果]
```

**欧氏距离公式**：
(a1−b1)2+(a2−b2)2+⋯+(an−bn)2(*a*1​−*b*1​)2+(*a*2​−*b*2​)2+⋯+(*a**n*​−*b**n*​)2​

### 2.2 Scikit-learn API

python

```
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(
    n_neighbors=5,        # K值
    algorithm='auto'      # 算法：'auto','ball_tree','kd_tree','brute'
)
knn.fit(X_train, y_train)
predictions = knn.predict(X_test)
```

### 2.3 K值选择问题

| K值大小   | 优点         | 缺点                   |
| :-------- | :----------- | :--------------------- |
| **小K值** | 捕捉局部模式 | 对噪声敏感，易过拟合   |
| **大K值** | 减少噪声影响 | 忽略局部特征，易欠拟合 |

**经验选择**：

- 使用交叉验证选择最优K值（通常3-10之间）

### 2.4 KNN优缺点

**优点**：

- 简单直观，无需训练
- 适用于非线性分类
- 对异常值不敏感

**缺点**：

- 计算复杂度高（O(n)）
- 内存消耗大
- 高维数据效果差（维数灾难）

## 3. 朴素贝叶斯算法

### 3.1 贝叶斯定理

P(A∣B)=P(B∣A)⋅P(A)P(B)*P*(*A*∣*B*)=*P*(*B*)*P*(*B*∣*A*)⋅*P*(*A*)

**文本分类应用**：
P(类别∣特征)∝P(类别)⋅∏P(特征∣类别)*P*(类别∣特征)∝*P*(类别)⋅∏*P*(特征∣类别)

### 3.2 拉普拉斯平滑

解决零概率问题：
P(特征i∣类别c)=Nic+αNc+αn*P*(特征*i*​∣类别*c*​)=*N**c*​+*α**n**N**i**c*​+*α*​
其中：

- $N_{ic}$：特征i在类别c中出现次数
- $N_c$：类别c的总词频
- $n$：特征总数
- $\alpha$：平滑系数（通常为1）

### 3.3 Scikit-learn API

python

```
from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB(alpha=1.0)  # 拉普拉斯平滑系数
nb.fit(X_train, y_train)
```

### 3.4 优缺点

**优点**：

- 计算效率高
- 对小规模数据表现好
- 适合文本分类

**缺点**：

- 特征独立性假设不成立时效果差
- 对输入数据表达敏感

## 4. 模型评估方法

### 4.1 混淆矩阵

|              | 预测为正类 | 预测为负类 |
| :----------- | :--------- | :--------- |
| **实际正类** | TP         | FN         |
| **实际负类** | FP         | TN         |

### 4.2 关键指标

| 指标     | 公式                                    | 意义                     |
| :------- | :-------------------------------------- | :----------------------- |
| 准确率   | (TP+TN)/(TP+FP+FN+TN)                   | 总体正确率               |
| 精确率   | TP/(TP+FP)                              | 查准率                   |
| 召回率   | TP/(TP+FN)                              | 查全率（医疗领域重视）   |
| F1-Score | 2*(Precision*Recall)/(Precision+Recall) | 精确率和召回率的调和平均 |

### 4.3 ROC与AUC

图表

代码

```
graph LR
    A[调整分类阈值] --> B[计算TPR和FPR]
    B --> C[绘制ROC曲线]
    C --> D[计算AUC面积]
```

- **AUC=1**：完美分类器
- **AUC=0.5**：随机猜测
- **AUC>0.8**：模型效果良好

## 5. 决策树与随机森林

### 5.1 决策树核心概念

**划分标准**：

1. **信息增益**（ID3算法）：
   Gain(D,A)=H(D)−H(D∣A)*G**ain*(*D*,*A*)=*H*(*D*)−*H*(*D*∣*A*)
2. **信息增益比**（C4.5算法）
3. **基尼系数**（CART算法）：
   Gini(p)=1−∑k=1Kpk2*G**ini*(*p*)=1−∑*k*=1*K*​*p**k*2​

### 5.2 Scikit-learn API

python

```
from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier(
    criterion='gini',     # 划分标准：'gini'或'entropy'
    max_depth=5,          # 树的最大深度
    random_state=42
)
```

### 5.3 随机森林

**算法流程**：

1. 随机选择样本（有放回抽样）
2. 随机选择特征子集
3. 构建多棵决策树
4. 投票决定最终结果

python

```
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,     # 树的数量
    max_depth=10,
    random_state=42
)
```

### 5.4 决策树 vs 随机森林

| 特性           | 决策树       | 随机森林         |
| :------------- | :----------- | :--------------- |
| **过拟合风险** | 高           | 低               |
| **准确性**     | 一般         | 高               |
| **训练速度**   | 快           | 慢（并行可加速） |
| **可解释性**   | 强（可视化） | 弱               |
| **特征重要性** | 支持         | 支持且更稳定     |

## 6. 模型选择与调优

### 6.1 交叉验证（Cross-Validation）

图表

代码

```
graph TD
    A[原始数据] --> B[划分5份]
    B --> C1[训练集1-4]
    C1 --> D1[验证集5]
    B --> C2[训练集1-3,5]
    C2 --> D2[验证集4]
    B --> C3[...]
```

### 6.2 网格搜索（GridSearchCV）

python

```
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_neighbors': [3,5,7,9],
    'weights': ['uniform', 'distance']
}

grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)
print(grid.best_params_)
```

## 7. 关键总结

### 算法选择指南

| 场景                   | 推荐算法      |
| :--------------------- | :------------ |
| 小数据集快速原型       | 朴素贝叶斯    |
| 高维稀疏数据（如文本） | 朴素贝叶斯    |
| 需要模型解释性         | 决策树        |
| 平衡精度与鲁棒性       | 随机森林      |
| 非结构化数据           | KNN（小规模） |

### 实践建议

1. **数据预处理**：标准化/归一化（KNN必需）
2. **特征工程**：特征选择提升效率
3. **模型评估**：使用多种指标综合评估
4. **超参数调优**：网格搜索+交叉验证
5. **集成方法**：随机森林通常是首选基准模型